#!/bin/bash
set -euo pipefail

# =================================================================
# 1. âš™ï¸ ì „ì—­ ì„¤ì • ë° ë³€ìˆ˜ ì„ ì–¸ (ìµœìƒìœ„ ë³€ìˆ˜ ìœ ì§€ ë° ì¬í™œìš©)
# =================================================================

# í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´
PROJECT_NAME="log-analyzer-agent"
NODE_VERSION="22"
PORT="13333"

# Docker ì„¤ì •
DOCKER_IMAGE_NAME="${PROJECT_NAME}"
DOCKER_CONTAINER_NAME="${PROJECT_NAME}-container"
DOCKER_NETWORK="${PROJECT_NAME}-net"
DOCKER_BASE_IMAGE="node:${NODE_VERSION}-alpine"

# AI ë° ì„œë¹„ìŠ¤ ì„¤ì •
# Gemini ì ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ëª¨ë¸: gemini-2.5-pro (ê³ ì„±ëŠ¥, ê³ ë¹„ìš©), gemini-2.5-flash (ê¸°ë³¸ ê¶Œì¥)
GEMINI_MODEL="gemini-2.5-flash"

# Claude ì ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ëª¨ë¸ (2025ë…„ 9ì›” ê¸°ì¤€):
# - ìµœê³  ì„±ëŠ¥: claude-3-opus-20240229, claude-3-5-sonnet (ìµœì‹ , ê· í˜•)
# - ê· í˜•/í‘œì¤€: claude-3-sonnet-20240229
# - ë¹ ë¥´ê³  ê²½ëŸ‰: claude-3-haiku-20240307 (ê¸°ë³¸ ê¶Œì¥)
CLAUDE_MODEL="claude-4-5-sonnet"

# Ollama ì ìš© ê°€ëŠ¥í•œ ë‹¤ë¥¸ ëª¨ë¸ ì˜ˆì‹œ (ë¡œì»¬ ì„¤ì¹˜ëœ ëª¨ë¸ëª…ì„ ì‚¬ìš©):
# - ê³ ì„±ëŠ¥/í‘œì¤€: llama3:8b (ê¸°ë³¸ ê¶Œì¥), llama3.2:1b (ê²½ëŸ‰)
# - ê²½ëŸ‰/ë¹ ë¥¸ ë¶„ë¥˜: qwen2.5:0.5b, qwen2.5:1.5b, phi3:mini, gemma:2b
OLLAMA_MODEL="qwen2.5:0.5b"
OLLAMA_HOST="http://192.168.0.100:11434"
BATCH_ENGINE="gemini" 

# ì‹œìŠ¤í…œ ì„¤ì •
MAX_CONCURRENT_REQUESTS="5"
LOG_LEVEL="info"
BATCH_INTERVAL="*/30 * * * *"
MAX_LOG_FILES="10"
MAX_LOG_SIZE_MB="0.1"
STREAM_CHUNK_SIZE="65536"
BATCH_SIZE="100"

# ë¯¼ê° ì •ë³´ (Placeholder)
GEMINI_API_KEY_PLACEHOLDER="AIzaSyDnfexvMNkS-"
CLAUDE_API_KEY_PLACEHOLDER="sk-ant-api03-"
MATTERMOST_WEBHOOK_URL_PLACEHOLDER="https://mattermost.org/hooks/"

# ì¢…ì†ì„± ë²„ì „
EXPRESS_VERSION="^4.21.2"
AXIOS_VERSION="^1.7.8"
WINSTON_VERSION="^3.16.0"
JOI_VERSION="^17.9.1"
GLOB_VERSION="^10.4.5"
CORS_VERSION="^2.8.5"
HELMET_VERSION="^8.1.0"
NODE_SCHEDULE_VERSION="^2.1.1"
P_LIMIT_VERSION="^5.0.0"
GOOGLE_GEMINI_SDK_VERSION="^0.17.0"

# ë””ë ‰í† ë¦¬ ê²½ë¡œ
BASE_DIR="$(pwd)/${PROJECT_NAME}"
SRC_DIR="${BASE_DIR}/src"
LOGS_DIR="${BASE_DIR}/logs"
SCRIPTS_DIR="${BASE_DIR}/scripts"
VOLUMES_DIR="${BASE_DIR}/volumes"
CONFIG_DIR="${VOLUMES_DIR}/config"
STATE_DIR="${VOLUMES_DIR}/state"


# =================================================================
# 2. ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë° ì´ˆê¸°í™”
# =================================================================

echo "ğŸš€ 1ë‹¨ê³„: í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ë° ì´ˆê¸° íŒŒì¼ ìƒì„± ì‹œì‘..."

rm -rf "$BASE_DIR" # ì´ì „ ë””ë ‰í† ë¦¬ ì™„ì „ ì‚­ì œ

# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$BASE_DIR"
mkdir -p "${SRC_DIR}"/{utils,services,routes,middleware}
mkdir -p "${LOGS_DIR}"/{nginx,api-server,auth-service}
mkdir -p "$SCRIPTS_DIR"
mkdir -p "$CONFIG_DIR"
mkdir -p "$STATE_DIR"
echo "   -> ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ."


# =================================================================
# 3. ğŸ“ config.json íŒŒì¼ ìƒì„± (ì‚¬ìš©ìê°€ ì œê³µí•œ ë‚´ìš© ê·¸ëŒ€ë¡œ ë°˜ì˜)
# =================================================================

echo "3. config.json (ì „ì—­ ì„¤ì • íŒŒì¼) ìƒì„± ì¤‘..."
cat << EOF_CONFIG_JSON > "${CONFIG_DIR}/config.json"
{
  "api": {
    "port": ${PORT},
    "logLevel": "${LOG_LEVEL}",
    "maxConcurrentRequests": ${MAX_CONCURRENT_REQUESTS}
  },
  "logProcessing": {
    "maxLogFiles": ${MAX_LOG_FILES},
    "maxLogSizeMB": ${MAX_LOG_SIZE_MB},
    "streamChunkSize": ${STREAM_CHUNK_SIZE},
    "batchSize": ${BATCH_SIZE},
    "batchInterval": "${BATCH_INTERVAL}",
    "batchEngine": "${BATCH_ENGINE}"
  },
  "appLogPaths": {
    "nginx": [
      "/var/logs/nginx/*.log"
    ],
    "api-server": [
      "/var/logs/api-server/*.log"
    ]
  },
  "ai": {
    "geminiApiKey": "${GEMINI_API_KEY_PLACEHOLDER}",
    "claudeApiKey": "${CLAUDE_API_KEY_PLACEHOLDER}",
    "ollamaHost": "${OLLAMA_HOST}",
    "geminiModel": "${GEMINI_MODEL}",
    "claudeModel": "${CLAUDE_MODEL}",
    "ollamaModel": "${OLLAMA_MODEL}"
  },
  "mattermost": {
    "webhookUrl": "${MATTERMOST_WEBHOOK_URL_PLACEHOLDER}",
    "enabled": true
  }
}
EOF_CONFIG_JSON
echo "   -> volumes/config/config.json ìƒì„± ì™„ë£Œ."


# =================================================================
# 4. ğŸ“ ë©”íƒ€ íŒŒì¼ ìƒì„± ë° Docker Compose
# =================================================================

# .gitignore íŒŒì¼
cat << 'EOF_GITIGNORE' > "${BASE_DIR}/.gitignore"
node_modules/
dist/
.env
volumes/state/*
EOF_GITIGNORE

# package.json íŒŒì¼
cat << EOF_PACKAGE_JSON > "${BASE_DIR}/package.json"
{
    "name": "${PROJECT_NAME}",
    "version": "1.0.0",
    "description": "Multi-LLM Log Analysis AI Agent with Mattermost reporting",
    "main": "src/index.js",
    "scripts": {
        "start": "node src/index.js"
    },
    "dependencies": {
        "express": "${EXPRESS_VERSION}",
        "axios": "${AXIOS_VERSION}",
        "winston": "${WINSTON_VERSION}",
        "joi": "${JOI_VERSION}",
        "glob": "${GLOB_VERSION}",
        "cors": "${CORS_VERSION}",
        "helmet": "${HELMET_VERSION}",
        "node-schedule": "${NODE_SCHEDULE_VERSION}",
        "p-limit": "${P_LIMIT_VERSION}",
        "@google/generative-ai": "${GOOGLE_GEMINI_SDK_VERSION}"
    },
    "author": "",
    "license": "ISC"
}
EOF_PACKAGE_JSON

# Dockerfile
cat << EOF_DOCKERFILE > "${BASE_DIR}/Dockerfile"
FROM ${DOCKER_BASE_IMAGE} AS base

RUN apk add --no-cache bash

WORKDIR /app

COPY package*.json ./
RUN npm install --omit=dev

COPY src src
# config.json ë° state.jsonì€ ë³¼ë¥¨ ë§ˆìš´íŠ¸ë˜ë¯€ë¡œ ë³µì‚¬í•˜ì§€ ì•ŠìŒ

# ë¡œê·¸ íŒŒì¼ ì ‘ê·¼ ê²½ë¡œ (ë³¼ë¥¨ ë§ˆìš´íŠ¸ì˜ ëŒ€ìƒì´ ë¨)
RUN mkdir -p /var/logs

EXPOSE ${PORT}
CMD [ "npm", "start" ]
EOF_DOCKERFILE

# docker-compose.yml íŒŒì¼
cat << EOF_DOCKER_COMPOSE > "${BASE_DIR}/docker-compose.yml"
services:
  ${PROJECT_NAME}:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: ${DOCKER_CONTAINER_NAME}
    image: ${DOCKER_IMAGE_NAME}
    restart: unless-stopped
    networks:
      - ${DOCKER_NETWORK}
    ports:
      - "${PORT}:${PORT}"
    
    volumes:
      # 1. ì„¤ì • íŒŒì¼ ìœ ë™ì„± í™•ë³´: í˜¸ìŠ¤íŠ¸ config.jsonì„ ì»¨í…Œì´ë„ˆ ë£¨íŠ¸ë¡œ ë§ˆìš´íŠ¸
      - ./volumes/config/config.json:/app/config.json 
      
      # 2. ìƒíƒœ íŒŒì¼ ì˜ì†ì„± í™•ë³´
      - ./volumes/state:/app/state 
      
      # 3. ë¡œê·¸ íŒŒì¼ ì ‘ê·¼ (í˜¸ìŠ¤íŠ¸ logs ë””ë ‰í† ë¦¬ë¥¼ ì»¨í…Œì´ë„ˆì˜ /var/logsë¡œ ë§ˆìš´íŠ¸)
      - /opt/npmplus/nginx:/var/logs/nginx
    
networks:
  ${DOCKER_NETWORK}:
    driver: bridge
EOF_DOCKER_COMPOSE

# ë¡œê·¸ íŒŒì¼ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
cat << 'EOF_LOG1' > "${LOGS_DIR}/nginx/access.log"
2025-09-28 10:00:01 [INFO] Nginx: GET /index.html 200 (File exists)
2025-09-28 10:00:02 [ERROR] Nginx: Backend service timeout on /data endpoint.
EOF_LOG1
cat << 'EOF_LOG2' > "${LOGS_DIR}/nginx/error_critical.log"
2025-09-28 10:00:05 [CRITICAL] Nginx: Configuration reload failed, using old settings.
EOF_LOG2
cat << 'EOF_LOG3' > "${LOGS_DIR}/api-server/server.log"
2025-09-28 10:00:07 [WARN] APIServer: Database query slow (550ms).
EOF_LOG3

echo "âœ… 1ë‹¨ê³„ ì™„ë£Œ: ë©”íƒ€ íŒŒì¼ ë° ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„± ì™„ë£Œ."

## 5. ğŸ’» Node.js ì†ŒìŠ¤ ì½”ë“œ ìƒì„± (ëª¨ë“  ì†ŒìŠ¤ íŒŒì¼ í¬í•¨)

echo "ğŸ’» 2ë‹¨ê³„: í•µì‹¬ ì†ŒìŠ¤ ì½”ë“œ (ëª¨ë“  íŒŒì¼) ìƒì„± ì‹œì‘..."

# 1. src/utils/config.js (config.json ë¡œë“œ)
cat << 'EOF_CONFIG' > "${SRC_DIR}/utils/config.js"
// src/utils/config.js (JSON ë¡œë“œ ë²„ì „)

const path = require('path');
const fs = require('fs');

// ë§ˆìš´íŠ¸ëœ config.json íŒŒì¼ì˜ ê²½ë¡œ (ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
const CONFIG_FILE_PATH = path.resolve(__dirname, '../../config.json'); 
let config = {};

try {
    // ë™ê¸°ì ìœ¼ë¡œ JSON íŒŒì¼ ë¡œë“œ
    const rawData = fs.readFileSync(CONFIG_FILE_PATH, 'utf8');
    config = JSON.parse(rawData);
    
    // ê¸°ë³¸ ì•± ëª©ë¡ì„ appLogPathsì˜ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ êµ¬ì„±
    config.defaultApps = Object.keys(config.appLogPaths)
        .filter(app => config.appLogPaths[app].length > 0);

} catch (error) {
    console.error(`ğŸš¨ FATAL: Could not load or parse config file at ${CONFIG_FILE_PATH}`);
    console.error(`Error details: ${error.message}`);
    process.exit(1); 
}

module.exports = config;
EOF_CONFIG

# 2. src/utils/logger.js (config.jsonì—ì„œ ë ˆë²¨ ì„¤ì •)
cat << 'EOF_LOGGER' > "${SRC_DIR}/utils/logger.js"
const { createLogger, format, transports } = require('winston');
const config = require('./config');

const logger = createLogger({
    // ğŸ’¡ config.jsonì—ì„œ ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    level: config.api.logLevel || 'info',
    format: format.combine(
        format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
        format.errors({ stack: true }),
        format.splat(),
        format.json()
    ),
    transports: [
        new transports.Console({
            format: format.combine(format.colorize(), format.simple())
        })
    ],
    exceptionHandlers: [
        new transports.Console({
            format: format.combine(format.colorize(), format.simple())
        })
    ]
});

module.exports = logger;
EOF_LOGGER

# 3. src/utils/validator.js
cat << 'EOF_VALIDATOR' > "${SRC_DIR}/utils/validator.js"
const Joi = require('joi');

const analyzeSchema = Joi.object({
    apps: Joi.array().items(Joi.string()).min(1).required(),
    engine: Joi.string().valid('gemini', 'ollama', 'claude').required()
});

const validateAnalyzeRequest = (data) => {
    return analyzeSchema.validate(data);
};

module.exports = {
    validateAnalyzeRequest
};
EOF_VALIDATOR

# 4. src/services/stateManager.js (ë³¼ë¥¨ ë§ˆìš´íŠ¸ë¥¼ ìœ„í•´ ì¸ë©”ëª¨ë¦¬ ë°©ì‹ ìœ ì§€)
cat << 'EOF_STATE_MANAGER' > "${SRC_DIR}/services/stateManager.js"
const logger = require('../utils/logger');
const fs = require('fs');
const path = require('path');

// ğŸ’¡ ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ ìƒíƒœ íŒŒì¼ ê²½ë¡œ
const STATE_FILE = path.resolve(__dirname, '../../state/state.json');
let logAnalysisState = {};

const loadState = () => {
    if (!fs.existsSync(STATE_FILE)) {
        logger.info('[StateManager] State file not found. Initializing new state.');
        return;
    }
    try {
        const rawData = fs.readFileSync(STATE_FILE, 'utf8');
        logAnalysisState = JSON.parse(rawData);
        logger.info(`[StateManager] State loaded successfully. ${Object.keys(logAnalysisState).length} markers found.`);
    } catch (error) {
        logger.error(`[StateManager] Failed to load state: ${error.message}. Resetting to empty state.`);
        logAnalysisState = {};
    }
};

const saveState = () => {
    try {
        const dir = path.dirname(STATE_FILE);
        if (!fs.existsSync(dir)) {
            fs.mkdirSync(dir, { recursive: true });
        }
        fs.writeFileSync(STATE_FILE, JSON.stringify(logAnalysisState, null, 2), 'utf8');
    } catch (error) {
        logger.error(`[StateManager] Failed to save state to ${STATE_FILE}: ${error.message}`);
    }
};

// --- API ---

const getLastMarker = (logIdentifier) => {
    return logAnalysisState[logIdentifier] || 0;
};

const updateMarker = (logIdentifier, newMarker) => {
    if (newMarker > getLastMarker(logIdentifier)) {
        logAnalysisState[logIdentifier] = newMarker;
        // ğŸ’¡ ë§ˆì»¤ ì—…ë°ì´íŠ¸ ì‹œ íŒŒì¼ì— ì €ì¥
        saveState(); 
        logger.debug(`[StateManager] Updated marker for ${logIdentifier} to ${newMarker}`);
    }
};

const resetState = () => {
    const previousStateCount = Object.keys(logAnalysisState).length;
    logAnalysisState = {};
    saveState(); // ì´ˆê¸°í™” í›„ íŒŒì¼ì— ì €ì¥
    logger.info(`[StateManager] Log analysis state has been reset. Cleared ${previousStateCount} markers.`);
    return previousStateCount;
};

const getStateSnapshot = () => {
    return { ...logAnalysisState };
};

// ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œ ìƒíƒœ ë¡œë“œ
loadState();

module.exports = {
    getLastMarker,
    updateMarker,
    resetState,
    getStateSnapshot,
};
EOF_STATE_MANAGER

cat << 'EOF_REPORT_GENERATOR' > "${SRC_DIR}/services/reportGenerator.js"
const logger = require('../utils/logger');

/**
 * AI ë¶„ì„ ê²°ê³¼ë¥¼ ìµœì¢… ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•˜ê±°ë‚˜ íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
 * í˜„ì¬ëŠ” ì½˜ì†”ì— ì¶œë ¥í•˜ëŠ” ì—­í• ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
 * @param {string} analysisResult logAnalyzer.jsì—ì„œ ë°˜í™˜ëœ Markdown ë¶„ì„ ê²°ê³¼
 */
const generateAndOutputReport = async (analysisResult) => {
    logger.info('--- ğŸ“Š AI Log Analysis Report (Markdown Table) ---');
    console.log(analysisResult);
    logger.info('--- End of Report ---');
};

module.exports = {
    generateAndOutputReport,
};
EOF_REPORT_GENERATOR

# 5. src/services/logAnalyzer.js
cat << 'EOF_LOG_ANALYZER' > "${SRC_DIR}/services/logAnalyzer.js"
const { GoogleGenerativeAI } = require('@google/generative-ai');
const axios = require('axios');
const logger = require('../utils/logger');
const config = require('../utils/config');

// config.jsonì˜ êµ¬ì¡°ê°€ ë‹¤ë¥´ë¯€ë¡œ, ì´ì „ì— ì‚¬ìš©ëœ config êµ¬ì¡°ì— ë§ì¶° ì„ì‹œì ìœ¼ë¡œ ê°’ì„ ì¡°ì •
// analyzeLogs í•¨ìˆ˜ì˜ ì¸ìˆ˜ì— ë§ì¶”ê¸° ìœ„í•´ config.ai êµ¬ì¡° ëŒ€ì‹  config.gemini ë“±ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

// Markdown í…Œì´ë¸” ì¶”ì¶œê¸°: ìµœì†Œí•œì˜ ìœ íš¨ì„± ê²€ì‚¬ë§Œ ìˆ˜í–‰
const extractMarkdownTable = (text) => {
    if (!text || typeof text !== 'string') return '';
    
    const rawText = text.trim();
    const lines = rawText.split(/\r?\n/).filter(line => line.trim() !== '');
    if (lines.length < 2) return ''; // ìµœì†Œí•œ ë‘ ì¤„ (í—¤ë”ì™€ êµ¬ë¶„ì„ ) í•„ìš”
    return rawText;
};

// Gemini ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
const getFirstAvailableText = (resp) => {
    if (!resp) return null;

    // ìµœìƒìœ„ text í™•ì¸ (Google SDKì˜ ê²½ìš° response.text ì‚¬ìš©)
    const candidates = resp.candidates || [];
    for (const candidate of candidates) {
        const parts = candidate?.content?.parts || [];
        for (const part of parts) {
            if (typeof part?.text === 'string' && part.text.trim() !== '') {
                return part.text.trim();
            }
        }
    }

    return null;
};


const analyzeLogs = async (engine, logContent) => {
    // ğŸ’¡ ì°¸ê³ : config.jsonì´ config.ai êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, 
    // ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ config.gemini ëŒ€ì‹  config.aië¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
    const GEMINI_API_KEY = config.ai.geminiApiKey;
    const GEMINI_MODEL = config.ai.geminiModel;
    const CLAUDE_API_KEY = config.ai.claudeApiKey;
    const CLAUDE_MODEL = config.ai.claudeModel;
    const OLLAMA_HOST = config.ai.ollamaHost;
    const OLLAMA_MODEL = config.ai.ollamaModel;

    const prompt = `ë‹¹ì‹ ì€ ë¡œê·¸ ë¶„ì„ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. ë‹¤ìŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”.
ì˜¤ë¥˜, ê²½ê³ , ì„±ëŠ¥ ë¬¸ì œ, ì£¼ìš” ë³´ì•ˆ ì´ë²¤íŠ¸ì— ì§‘ì¤‘í•˜ì„¸ìš”.
ë¡œê·¸ëŠ” ì—¬ëŸ¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ê°€ì ¸ì˜¨ ê²ƒì´ë©°, ê° ê²½ë¡œê°€ ëª…í™•í•˜ê²Œ í‘œì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤.
AI ì‘ë‹µì€ ë°˜ë“œì‹œ **Markdown í‘œ** í˜•íƒœë¡œë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, **ì¶”ê°€ ì„¤ëª…, í—¤ë”, í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”**.
í‘œëŠ” ë‹¤ìŒ ì—´ì„ ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤ (ìƒíƒœì—ëŠ” ì ì ˆí•œ ì´ëª¨ì§€ ì‚¬ìš©):
| ë¡œê·¸ ê²½ë¡œ | ìƒíƒœ | ì„¤ëª… | ê¶Œì¥ ì¡°ì¹˜ |
|-------------------------|----------|--------------|--------------|
| /var/logs/app/file1.log | ğŸ”´ ë†’ìŒ | ì˜¤ë¥˜ ì„¸ë¶€ ì •ë³´ | í•´ê²°ì±… |

Analyze this log content:
---LOG START---
${logContent}
---LOG END---`;

    try {
        let rawText = null;

        if (engine === 'gemini') {
            if (!GEMINI_API_KEY) throw new Error('GEMINI_API_KEY is not configured.');

            // ğŸ’¡ FIX: ì´ì „ ì •ìƒ ì‘ë™ ë°©ì‹ (GoogleGenerativeAI, í‚¤ë¥¼ ë¬¸ìì—´ë¡œ ì „ë‹¬)ìœ¼ë¡œ ë³µì›
            const genAI = new GoogleGenerativeAI(GEMINI_API_KEY);
            const model = genAI.getGenerativeModel({ model: GEMINI_MODEL });

            logger.info(`[Gemini] Requesting analysis with model: ${GEMINI_MODEL}`);
            const response = await model.generateContent(prompt);
            
            // SDKì˜ response.textê°€ ë¹„ì–´ìˆì„ ê²½ìš° candidatesì—ì„œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
            rawText = response.text || getFirstAvailableText(response.response);
            
            if (!rawText) {
                const finishReason = response?.response?.candidates?.[0]?.finishReason || 'UNKNOWN';
                if (finishReason === 'SAFETY') throw new Error(`Gemini blocked by safety settings.`);
            }

        } else if (engine === 'claude') {
            if (!CLAUDE_API_KEY) throw new Error('CLAUDE_API_KEY is not configured.');
            
            const claudeUrl = 'https://api.anthropic.com/v1/messages';
            logger.info(`[Claude] Requesting analysis with model: ${CLAUDE_MODEL}`);
            
            const response = await axios.post(claudeUrl, {
                model: CLAUDE_MODEL,
                max_tokens: 4096,
                messages: [{ role: 'user', content: prompt }],
                stop_sequences: ["\n\n"], // ì¶”ê°€ í…ìŠ¤íŠ¸ ë°©ì§€ ìœ ë„
            }, {
                headers: {
                    'x-api-key': CLAUDE_API_KEY,
                    'anthropic-version': '2023-06-01',
                    'content-type': 'application/json'
                }
            });

            if (response.data?.content?.length > 0) {
                rawText = response.data.content[0].text;
            }

            if (!rawText) {
                const stopReason = response.data?.stop_reason || 'UNKNOWN';
                if (stopReason === 'safety') throw new Error(`Claude blocked by safety settings.`);
            }

        } else if (engine === 'ollama') {
            const ollamaUrl = `${OLLAMA_HOST}/api/generate`;
            logger.info(`[Ollama] Requesting analysis with model ${OLLAMA_MODEL} at ${ollamaUrl}`);

            const response = await axios.post(ollamaUrl, {
                model: OLLAMA_MODEL,
                prompt,
                stream: false
            });

            rawText = response.data?.response;

        } else {
            throw new Error(`Unsupported AI engine: ${engine}`);
        }

        // --- ê³µí†µ í›„ì²˜ë¦¬ ë° ìœ íš¨ì„± ê²€ì‚¬ ---
        if (!rawText || rawText.trim() === '') {
            logger.error(`[${engine}] API returned no text.`);
            throw new Error(`${engine} API returned an empty text response.`);
        }
        
        const analysisText = extractMarkdownTable(rawText);

        if (!analysisText) {
            logger.error(`[${engine}] Failed to extract parsable Markdown table from response.`);
            throw new Error('AI analysis failed: AI did not return a parsable Markdown table as requested.');
        }

        return analysisText;

    } catch (error) {
        // Axios ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” (API í‚¤, ë„¤íŠ¸ì›Œí¬, 4xx/5xx ë“±)
        let message = error.message;
        if (error.response?.data?.error) {
            message = error.response.data.error.message || message;
        } else if (error.response?.data?.type) {
            message = error.response.data.type; // Claude ì˜¤ë¥˜ ìœ í˜•
        }
        logger.error(`[${engine}] AI analysis failed: ${message}`);
        throw new Error(`AI analysis failed: ${message}`);
    }
};

module.exports = { analyzeLogs };
EOF_LOG_ANALYZER

# 7. src/services/streamProcessor.js (Glob íŒ¨í„´ ì²˜ë¦¬ ë¡œì§ í¬í•¨)
cat << 'EOF_STREAM_PROCESSOR_V3' > "${SRC_DIR}/services/streamProcessor.js"
const fs = require('fs');
const path = require('path');
const logger = require('../utils/logger');
const stateManager = require('./stateManager');
const config = require('../utils/config');
const { globSync } = require('glob');

/**
 * ì£¼ì–´ì§„ ê²½ë¡œ íŒ¨í„´ ë°°ì—´ì„ Globìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì‹¤ì œ íŒŒì¼ ê²½ë¡œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
 * @param {string[]} pathPatterns - config.jsonì—ì„œ ì •ì˜ëœ íŒ¨í„´ ë°°ì—´
 * @returns {string[]} ì‹¤ì œ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ë°°ì—´
 */
const findLogFiles = (pathPatterns) => {
    let files = new Set();
    
    for (let pattern of pathPatterns) {
        // 1. ë””ë ‰í† ë¦¬ ì²˜ë¦¬ (ê²½ë¡œê°€ '/' ë˜ëŠ” '\'ë¡œ ëë‚˜ë©´ í•´ë‹¹ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ì°¾ìŒ)
        if (pattern.endsWith(path.sep) || pattern.endsWith('/')) {
            pattern = path.join(pattern, '*'); // ë””ë ‰í† ë¦¬ íŒ¨í„´ì„ Glob íŒ¨í„´ìœ¼ë¡œ ë³€í™˜ (ëª¨ë“  íŒŒì¼)
        }
        
        // 2. Glob íŒ¨í„´ ì²˜ë¦¬
        try {
            // nodir: trueëŠ” ë””ë ‰í† ë¦¬ë¥¼ ê²°ê³¼ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.
            const foundFiles = globSync(pattern, { nodir: true });
            foundFiles.forEach(file => files.add(file));
        } catch(error) {
            logger.error(`[Stream] Invalid glob pattern: ${pattern}. Error: ${error.message}`);
        }
    }
    
    return Array.from(files);
};

/**
 * ë‹¨ì¼ ë¡œê·¸ íŒŒì¼ì—ì„œ ìƒˆë¡œìš´ ë‚´ìš©ì„ ì½ê³  ë§ˆì»¤ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
 */
const processSingleLogFileStream = (appName, logFilePath) => {
    return new Promise((resolve, reject) => {
        const logIdentifier = `${appName}:${logFilePath}`;
        let stats;
        
        // --- 1. ìƒíƒœ ë° íŒŒì¼ í¬ê¸° ì´ˆê¸° ì„¤ì • ---
        let lastMarker = stateManager.getLastMarker(logIdentifier);
        let fileSize;

        try {
            stats = fs.statSync(logFilePath);
            fileSize = stats.size;
        } catch (error) {
            logger.warn(`[Stream] Log file not found or inaccessible: ${logFilePath}`);
            return resolve({ content: '', newMarker: 0, logIdentifier: logIdentifier });
        }
        
        const maxLogSizeInBytes = config.logProcessing.maxLogSizeMB * 1024 * 1024;


        // --- 2. ğŸ’¥ ë¡œí…Œì´ì…˜/Truncate ê°ì§€ ë° ë§ˆì»¤ ì¬ì¡°ì • (í•µì‹¬ ìˆ˜ì •) ---
        if (fileSize < lastMarker) {
            logger.warn(`[Stream] Log file ${logIdentifier} was rotated or truncated. Resetting marker from ${lastMarker} to 0.`);
            
            // ë§ˆì»¤ë¥¼ ì¦‰ì‹œ 0ìœ¼ë¡œ ë¦¬ì…‹í•˜ê³ , fileSizeì™€ lastMarkerë¥¼ í˜„ì¬ ê°’ìœ¼ë¡œ ì¬ì„¤ì •
            // stateManager.updateMarker(logIdentifier, 0); // ğŸš¨ ì£¼ì˜: stateManager ì €ì¥ í˜¸ì¶œì€ ìµœì¢… ì„±ê³µ ì‹œì—ë§Œ!
            lastMarker = 0; // ì´ ì¸ìŠ¤í„´ìŠ¤ì—ì„œë§Œ ë§ˆì»¤ë¥¼ 0ìœ¼ë¡œ ì‚¬ìš©
            
            // ì´ ì‹œì ë¶€í„°ëŠ” fileSize > lastMarker (0) ì´ë¯€ë¡œ ì•„ë˜ì˜ ì¼ë°˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ë”°ë¦„.
        }
        
        // --- 3. ì²˜ë¦¬ ì—¬ë¶€ ê²€ì‚¬ (ë§ˆì»¤ ì¬ì¡°ì • í›„ ë‹¤ì‹œ ê²€ì‚¬) ---
        if (fileSize === lastMarker) {
            logger.info(`[Stream] Skipping ${logIdentifier}: already processed (Size: ${fileSize}, Marker: ${lastMarker})`);
            return resolve({ content: '', newMarker: fileSize, logIdentifier: logIdentifier });
        }

        if (fileSize === 0) {
            logger.info(`[Stream] Skipping ${logIdentifier}: file is empty`);
            return resolve({ content: '', newMarker: 0, logIdentifier: logIdentifier });
        }


        // --- 4. ì½ì„ ì˜¤í”„ì…‹ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
        const newLogSize = fileSize - lastMarker;
        let readStartOffset = lastMarker;
        let readLength = newLogSize;

        if (newLogSize > maxLogSizeInBytes) {
            readStartOffset = Math.floor(fileSize - maxLogSizeInBytes);
            readLength = fileSize - readStartOffset; 
            
            logger.warn(
                `[Stream] New logs for ${logIdentifier} (${(newLogSize / 1024 / 1024).toFixed(2)}MB) exceed ` +
                `MAX_LOG_SIZE_MB (${config.logProcessing.maxLogSizeMB}MB). ` +
                `Reading only latest ${(readLength / 1024 / 1024).toFixed(2)}MB from file.`
            );
        }
        
        // --- 5. ìŠ¤íŠ¸ë¦¼ ìƒì„± ë° ì½ê¸° (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---
        const readStream = fs.createReadStream(logFilePath, {
            encoding: 'utf8',
            start: readStartOffset,
            end: fileSize - 1,
            highWaterMark: config.logProcessing.streamChunkSize 
        });

        let newLogContent = '';
        logger.info(
            `[Stream] Reading ${logIdentifier} from offset ${readStartOffset} to ${fileSize} ` +
            `(Reading ${(readLength / 1024).toFixed(2)}KB of ${(newLogSize / 1024).toFixed(2)}KB new content)`
        );

        readStream.on('data', (chunk) => {
            newLogContent += chunk.toString();
        });

        readStream.on('end', () => {
            const newMarker = fileSize;
            // ... (ìƒëµ: ë¡œê¹…)
            
            // ğŸš¨ ìµœì¢… ë§ˆì»¤ ì—…ë°ì´íŠ¸ëŠ” analyzeê°€ ì„±ê³µí–ˆì„ ë•Œ batchProcessor/analyze.jsì—ì„œ ìˆ˜í–‰í•¨.
            // ë¡œí…Œì´ì…˜ëœ ê²½ìš°(lastMarker=0), ì—¬ê¸°ì„œ ë°˜í™˜ë˜ëŠ” newMarkerëŠ” fileSizeê°€ ë˜ë©°,
            // analyze ì„±ê³µ ì‹œ stateManagerì— 0ì´ ì•„ë‹Œ fileSizeê°€ ì €ì¥ë©ë‹ˆë‹¤.
            const content = `--- ${logFilePath} (App: ${appName}) ---\n${newLogContent}\n\n`;
            
            resolve({ content, newMarker, logIdentifier });
        });

        readStream.on('error', (error) => {
            logger.error(`[Stream] Error reading ${logIdentifier}: ${error.message}`);
            reject(error);
        });
    });
};

/**
 * ğŸ’¡ ìµœìƒìœ„ í•¨ìˆ˜: ì£¼ì–´ì§„ ì•±ì— ëŒ€í•´ ì„¤ì •ëœ ëª¨ë“  ê²½ë¡œ íŒ¨í„´ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
 */
const processAppLogStreams = async (appName, logPathPatterns) => {
    // Globì„ ì‚¬ìš©í•˜ì—¬ íŒ¨í„´ì„ ì‹¤ì œ íŒŒì¼ ëª©ë¡ìœ¼ë¡œ í™•ì¥
    const filePaths = findLogFiles(logPathPatterns);
    
    if (filePaths.length === 0) {
        logger.info(`[Stream] No log files found for app: ${appName}`);
        return [];
    }
    
    // config.logProcessing.maxLogFiles ì œí•œ ì ìš©
    const filesToProcess = filePaths.slice(0, config.logProcessing.maxLogFiles);

    logger.info(`[Stream] Found ${filePaths.length} files, processing ${filesToProcess.length} for app ${appName}.`);
    
    const results = [];
    for (const logFilePath of filesToProcess) {
        try {
            const result = await processSingleLogFileStream(appName, logFilePath);
            // newMarkerê°€ 0ì´ ì•„ë‹ˆê±°ë‚˜, ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ê²°ê³¼ì— ì¶”ê°€
            if (result.content && result.content.trim()) {
                 results.push(result);
            }
        } catch (error) {
            logger.error(`[Stream] Failed to process file ${logFilePath}: ${error.message}`);
        }
    }
    
    return results;
}

module.exports = {
    processAppLogStreams,
};
EOF_STREAM_PROCESSOR_V3

cat << 'EOF_MESSENGER' > "${SRC_DIR}/utils/messenger.js"
const axios = require('axios');
const config = require('./config');
const logger = require('./logger');

/**
 * Mattermost ì›¹í›…ì„ í†µí•´ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
 *
 * @param {string} analysisResult logAnalyzerì—ì„œ ë°˜í™˜ëœ Markdown ë¶„ì„ ê²°ê³¼
 */
const sendReportToMattermost = async (analysisResult) => {
    // ğŸ’¡ FIX: config.messenger ëŒ€ì‹  config.mattermost ì‚¬ìš© (config.json êµ¬ì¡°ì— ë§ì¶¤)
    const mattermostConfig = config.mattermost || {};
    const webhookUrl = mattermostConfig.webhookUrl;
    // config.jsonì— channelIdê°€ ì—†ì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì•ˆì „í•˜ê²Œ ì ‘ê·¼í•˜ë„ë¡ ìˆ˜ì •
    const channelId = mattermostConfig.channelId; 

    if (!webhookUrl) {
        logger.warn("Mattermost Webhook URLì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ë³´ê³ ì„œë¥¼ ì „ì†¡í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (utils/config.js í™•ì¸ í•„ìš”)");
        return;
    }
    
    // Mattermost ë©”ì‹œì§€ í˜•ì‹ (ê°„ë‹¨í•œ í¬ë§·)
    const payload = {
        channel_id: channelId, // channelIdê°€ undefinedë¼ë„ Mattermostê°€ ì›¹í›… ê¸°ë³¸ ì±„ë„ë¡œ ì „ì†¡í•¨
        username: "Log Analyzer AI",
        icon_url: "https://example.com/ai_icon.png", // ì ì ˆí•œ ì•„ì´ì½˜ URLë¡œ ë³€ê²½ í•„ìš”
        text: "## ğŸ¤– AI ë¡œê·¸ ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ\n\n" + analysisResult
    };

    try {
        await axios.post(webhookUrl, payload);
        logger.info('[Messenger] AI Report successfully sent to Mattermost.');
    } catch (error) {
        logger.error(`[Messenger] Failed to send report to Mattermost: ${error.message}`);
        // Mattermost ì˜¤ë¥˜ì˜ ê²½ìš° ì‘ë‹µ ë³¸ë¬¸ì—ì„œ ì„¸ë¶€ ì •ë³´ í™•ì¸
        if (error.response) {
            logger.error(`[Messenger] Mattermost response status: ${error.response.status}, data: ${error.response.data}`);
        }
    }
};

module.exports = {
    sendReportToMattermost,
};
EOF_MESSENGER

# 8. src/services/batchProcessor.js
cat << 'EOF_BATCH_PROCESSOR_V3' > "${SRC_DIR}/services/batchProcessor.js"
const schedule = require('node-schedule');
const logger = require('../utils/logger');
const config = require('../utils/config');
const { analyzeLogs } = require('./logAnalyzer');
const { processAppLogStreams } = require('./streamProcessor'); // Glob í¬í•¨ëœ ìµœìƒìœ„ í•¨ìˆ˜
const { sendReportToMattermost } = require('../utils/messenger'); // ğŸ’¡ FIX: reportGenerator ëŒ€ì‹  messengerì—ì„œ ê°€ì ¸ì˜´
const stateManager = require('../services/stateManager');
const rawPLimit = require('p-limit');
const pLimit = rawPLimit.default || rawPLimit;

let batchJob = null;
const limit = pLimit(config.api.maxConcurrentRequests || 5);

/**
 * ë°°ì¹˜ ë¶„ì„ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
 */
const performBatchAnalysis = async (appsToAnalyze, engine) => {
    const allLogContent = [];
    const logMarkersToUpdate = []; // { logIdentifier, newMarker }
    const appPaths = config.appLogPaths;

    logger.info(`[Batch] Starting analysis for apps: ${appsToAnalyze.join(', ')} using engine: ${engine}`);

    const appProcessingJobs = appsToAnalyze
        .filter(appName => appPaths[appName] && appPaths[appName].length > 0)
        .map(appName => limit(async () => {
            const logPathPatterns = appPaths[appName]; // íŒ¨í„´ ë°°ì—´
            
            // ğŸ’¡ Glob íŒ¨í„´ ì²˜ë¦¬ ë° íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (streamProcessor.js)
            const results = await processAppLogStreams(appName, logPathPatterns);
            
            for (const { content, newMarker, logIdentifier } of results) {
                if (content && content.trim()) {
                    allLogContent.push(content);
                    logMarkersToUpdate.push({ logIdentifier, newMarker }); 
                }
            }
        }));
    
    await Promise.all(appProcessingJobs);

    if (allLogContent.length === 0) {
        logger.info('[Batch] No new log content found in batch to analyze.');
        return;
    }
    
    const combinedLogContent = allLogContent.join('');
    
    try {
        const analysisReport = await analyzeLogs(engine, combinedLogContent);
        
        // Mattermost ì „ì†¡ ì‹œ, ë¶„ì„ ê²°ê³¼ë§Œ ì¸ìˆ˜ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        await sendReportToMattermost(analysisReport); // ğŸ’¡ ì¸ìˆ˜ëŠ” analysisReportë§Œ ì‚¬ìš©

        // ğŸ’¡ ë¶„ì„ ì„±ê³µ ì‹œì—ë§Œ ë§ˆì»¤ ì—…ë°ì´íŠ¸
        logMarkersToUpdate.forEach(({ logIdentifier, newMarker }) => {
            stateManager.updateMarker(logIdentifier, newMarker);
        });

        logger.info(`[Batch] Analysis successful. ${logMarkersToUpdate.length} file(s) processed. Report sent.`);

    } catch (error) {
        logger.error(`[Batch] AI analysis or Mattermost submission failed: ${error.message}`);
    }
};


const startBatchProcessing = () => {
    if (batchJob) {
        logger.warn('Batch processing is already running.');
        return;
    }
    
    batchJob = schedule.scheduleJob(config.logProcessing.batchInterval, async () => {
        logger.info(`[Batch] Starting scheduled log analysis batch job...`);
        
        const allApps = Object.keys(config.appLogPaths);
        const engine = config.logProcessing.batchEngine; 
        
        try {
            await performBatchAnalysis(allApps, engine);
        } catch (error) {
            logger.error(`[Batch] Fatal error during batch processing: ${error.message}`);
        }
        logger.info('[Batch] Batch log analysis job finished.');
    });

    logger.info(`[Batch] Batch processing scheduled to run with interval: ${config.logProcessing.batchInterval} using engine: ${config.logProcessing.batchEngine}`);
};

const stopBatchProcessing = () => {
    if (batchJob) {
        batchJob.cancel();
        batchJob = null;
        logger.info('[Batch] Batch processing stopped.');
    }
};

module.exports = {
    startBatchProcessing,
    stopBatchProcessing
};

EOF_BATCH_PROCESSOR_V3

# 9. src/routes/analyze.js
cat << 'EOF_ANALYZE_ROUTE_V3' > "${SRC_DIR}/routes/analyze.js"
const express = require('express');
const rawPLimit = require('p-limit');
const pLimit = rawPLimit.default || rawPLimit;
const router = express.Router();

const { validateAnalyzeRequest } = require('../utils/validator');
const config = require('../utils/config');
const logger = require('../utils/logger');
const { analyzeLogs } = require('../services/logAnalyzer');
const { processAppLogStreams } = require('../services/streamProcessor'); // Glob í¬í•¨ëœ ìµœìƒìœ„ í•¨ìˆ˜
const { sendReportToMattermost } = require('../utils/messenger'); // ğŸ’¡ FIX: utils/messengerì—ì„œ ê°€ì ¸ì˜´
const stateManager = require('../services/stateManager');

const limit = pLimit(config.api.maxConcurrentRequests);

/**
 * POST /api/analyze
 */
router.post('/', async (req, res, next) => {
    const { error, value } = validateAnalyzeRequest(req.body);
    if (error) {
        return res.status(400).send({ message: 'Invalid request body', details: error.details });
    }

    const { apps, engine } = value;
    const allLogContent = [];
    const logMarkersToUpdate = [];
    let totalFilesChecked = 0;

    logger.info(`[API] Starting analysis for apps: ${apps.join(', ')} using engine: ${engine}`);

    const appPaths = config.appLogPaths;
    
    // ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬ ì‘ì—… ìƒì„± ë° ë³‘ë ¬ ì‹¤í–‰
    const logProcessingJobs = Object.keys(appPaths)
        .filter(appName => apps.includes(appName) && appPaths[appName].length > 0)
        .map(appName => limit(async () => {
            const logPathPatterns = appPaths[appName]; // íŒ¨í„´ ë°°ì—´
            
            // ğŸ’¡ Glob íŒ¨í„´ ì²˜ë¦¬ ë° íŒŒì¼ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ (streamProcessor.js)
            const results = await processAppLogStreams(appName, logPathPatterns);
            
            totalFilesChecked += results.length;

            for (const { content, newMarker, logIdentifier } of results) {
                if (content && content.trim()) {
                    allLogContent.push(content);
                    logMarkersToUpdate.push({ logIdentifier, newMarker }); 
                }
            }
        }));
    
    try {
        await Promise.all(logProcessingJobs);
    } catch(err) {
        logger.error(`A concurrency error occurred during file processing: ${err.message}`);
        return res.status(500).send({ message: 'Error during log file processing', details: err.message });
    }

    if (allLogContent.length === 0) {
        logger.info('No new log content found to analyze.');
        return res.status(200).send({ message: 'No new logs found for analysis.', totalFilesChecked });
    }
    
    const combinedLogContent = allLogContent.join('');
    
    try {
        const analysisReport = await analyzeLogs(engine, combinedLogContent);
        
        // Mattermost ì „ì†¡ ì‹œ, ë¶„ì„ ê²°ê³¼ë§Œ ì¸ìˆ˜ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.
        await sendReportToMattermost(analysisReport); // ğŸ’¡ ì¸ìˆ˜ëŠ” analysisReportë§Œ ì‚¬ìš©

        // ğŸ’¡ ë¶„ì„ì— ì„±ê³µí•˜ë©´ ìƒíƒœ ê´€ë¦¬ìì˜ ë§ˆì»¤ ì—…ë°ì´íŠ¸
        logMarkersToUpdate.forEach(({ logIdentifier, newMarker }) => {
            stateManager.updateMarker(logIdentifier, newMarker);
        });

        logger.info(`[API] Analysis successful and state updated. Report sent to Mattermost.`);
        
        // API ì‘ë‹µì—ëŠ” ë¶„ì„ ê²°ê³¼ë¥¼ ì§ì ‘ í¬í•¨
        return res.status(200).send({
            message: 'Analysis successful. Report sent to Mattermost.',
            analysisReport: analysisReport,
            processedFiles: logMarkersToUpdate.length
        });

    } catch (error) {
        logger.error(`[API] AI analysis/Mattermost failed: ${error.message}`);
        return res.status(500).send({ 
            message: `AI analysis/Mattermost failed: ${error.message}`,
            details: error.stack
        });
    }
});

module.exports = router;
EOF_ANALYZE_ROUTE_V3

# 10. src/routes/reset.js
cat << 'EOF_RESET_ROUTE' > "${SRC_DIR}/routes/reset.js"
const express = require('express');
const router = express.Router();
const stateManager = require('../services/stateManager');
const logger = require('../utils/logger');

/**
 * GET /api/state
 * í˜„ì¬ ë¡œê·¸ ë¶„ì„ ë§ˆì»¤ ìƒíƒœë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
 */
router.get('/', (req, res) => {
    const snapshot = stateManager.getStateSnapshot();
    res.status(200).send({
        message: 'Current log marker state snapshot.',
        state: snapshot,
        totalMarkers: Object.keys(snapshot).length
    });
});

/**
 * POST /api/state/reset
 * ëª¨ë“  ë¡œê·¸ ë¶„ì„ ë§ˆì»¤ ìƒíƒœë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
 */
router.post('/reset', (req, res) => {
    const clearedCount = stateManager.resetState();
    logger.warn(`[API] Log analysis state reset requested. Cleared ${clearedCount} markers.`);
    res.status(200).send({
        message: 'Log analysis state has been reset successfully.',
        clearedMarkers: clearedCount
    });
});

module.exports = router;
EOF_RESET_ROUTE

# 11. src/middleware/errorHandler.js
cat << 'EOF_ERROR_HANDLER' > "${SRC_DIR}/middleware/errorHandler.js"
const logger = require('../utils/logger');

/**
 * ì „ì—­ ì—ëŸ¬ í•¸ë“¤ëŸ¬
 */
const errorHandler = (err, req, res, next) => {
    logger.error('Unhandled Error:', {
        message: err.message,
        stack: err.stack,
        url: req.originalUrl,
        method: req.method
    });

    const statusCode = err.statusCode || 500;
    const message = err.message || 'Internal Server Error';

    res.status(statusCode).send({
        status: 'error',
        message: message
    });
};

module.exports = errorHandler;
EOF_ERROR_HANDLER


# 12. src/index.js (ë©”ì¸ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸)
cat << EOF_INDEX > "${SRC_DIR}/index.js"
const express = require('express');
const helmet = require('helmet');
const cors = require('cors');

const config = require('./utils/config');
const logger = require('./utils/logger');
const { startBatchProcessing } = require('./services/batchProcessor');
const errorHandler = require('./middleware/errorHandler');

const analyzeRouter = require('./routes/analyze');
const resetRouter = require('./routes/reset');

const app = express();
const PORT = config.api.port;

// ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
app.use(helmet());
app.use(cors());
app.use(express.json());
app.use(express.urlencoded({ extended: true }));

// ë¼ìš°í„° ì„¤ì •
app.use('/api/analyze', analyzeRouter);
app.use('/api/state', resetRouter); // ìƒíƒœ í™•ì¸ ë° ë¦¬ì…‹ìš©

// ê¸°ë³¸ í—¬ìŠ¤ ì²´í¬
app.get('/', (req, res) => {
    res.status(200).send('Log Analyzer Agent is running.');
});

// ì—ëŸ¬ í•¸ë“¤ë§ ë¯¸ë“¤ì›¨ì–´
app.use(errorHandler);

// ì„œë²„ ì‹œì‘
const server = app.listen(PORT, () => {
    logger.info(\`âœ… Server running on port \${PORT}\`);
    logger.info(\`ğŸ” Log Level: \${config.api.logLevel}\`);

    // ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ (cron ìŠ¤ì¼€ì¤„ë§)
    startBatchProcessing();
});

// Graceful Shutdown
process.on('SIGTERM', () => {
    logger.info('SIGTERM signal received: Closing HTTP server.');
    server.close(() => {
        logger.info('HTTP server closed. Exiting process.');
        process.exit(0);
    });
});
EOF_INDEX


# --- 6. ğŸ› ï¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ê¶Œí•œ ì„¤ì • ---
echo "âš™ï¸ 3ë‹¨ê³„: ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì‹œì‘..."

# scripts/build.sh
cat << 'EOF_BUILD' > "${SCRIPTS_DIR}/build.sh"
#!/bin/bash
set -euo pipefail

cd "$(dirname "$0")/.."
echo "--- ğŸ³ Docker ì´ë¯¸ì§€ ë¹Œë“œ ì‹œì‘ ---"
# config.jsonìœ¼ë¡œ ì„¤ì •ì´ ë¶„ë¦¬ë˜ì—ˆìœ¼ë¯€ë¡œ, .env íŒŒì¼ì€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
docker compose -f docker-compose.yml build --no-cache
echo "--- âœ… Docker ì´ë¯¸ì§€ ë¹Œë“œ ì™„ë£Œ ---"
EOF_BUILD
chmod +x "${SCRIPTS_DIR}/build.sh"

# scripts/start.sh
cat << EOF_START > "${SCRIPTS_DIR}/start.sh"
#!/bin/bash
set -euo pipefail

cd "\$(dirname "\$0")/.."

echo "--- ğŸš€ Docker Compose ì„œë¹„ìŠ¤ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ) ---"
docker compose -f docker-compose.yml up -d 
echo "--- âœ… ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ ---"
echo "ğŸ’¡ ì„¤ì • íŒŒì¼ ê²½ë¡œ: \$(pwd)/volumes/config/config.json"
echo "ğŸ’¡ ë¡œê·¸ë¥¼ í™•ì¸í•˜ë ¤ë©´: docker logs ${DOCKER_CONTAINER_NAME} -f"
echo "ğŸ’¡ ì„¤ì • ë³€ê²½ í›„ ì¬ì‹œì‘: docker compose restart"
echo "ğŸ’¡ ì„œë¹„ìŠ¤ ì¢…ë£ŒëŠ”: ./scripts/stop.sh"
EOF_START
chmod +x "${SCRIPTS_DIR}/start.sh"

# scripts/stop.sh
cat << EOF_STOP > "${SCRIPTS_DIR}/stop.sh"
#!/bin/bash
set -euo pipefail

DOCKER_CONTAINER_NAME="${DOCKER_CONTAINER_NAME}"
DOCKER_NETWORK="${DOCKER_NETWORK}"

cd "\$(dirname "\$0")/.."

echo "--- ğŸ›‘ Docker Compose ì„œë¹„ìŠ¤ ì¢…ë£Œ ë° ì»¨í…Œì´ë„ˆ ì‚­ì œ ---"
docker compose -f docker-compose.yml down
echo "--- âœ… ì„œë¹„ìŠ¤ ì¢…ë£Œ ì™„ë£Œ ---"

if docker network ls --format "{{.Name}}" | grep -q "${DOCKER_NETWORK}"; then
    echo "--- ğŸ—‘ï¸ ì‚¬ìš©ëœ Docker ë„¤íŠ¸ì›Œí¬ (${DOCKER_NETWORK}) ì •ë¦¬ ---"
    docker network rm "${DOCKER_NETWORK}" || true
fi
EOF_STOP
chmod +x "${SCRIPTS_DIR}/stop.sh"

echo "âœ… 3ë‹¨ê³„ ì™„ë£Œ: ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë° ê¶Œí•œ ì„¤ì •."
echo "================================================================="
echo "ğŸ‰ í”„ë¡œì íŠ¸ '${PROJECT_NAME}' ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
echo "ğŸ’¡ **í•µì‹¬ ìˆ˜ì • ì‚¬í•­**: ëˆ„ë½ë˜ì—ˆë˜ ëª¨ë“  Node.js ì†ŒìŠ¤ íŒŒì¼ (ì´ 12ê°œ)ì„ ìŠ¤í¬ë¦½íŠ¸ì— í¬í•¨í–ˆìŠµë‹ˆë‹¤."
echo "ğŸ’¡ **ì˜ˆìƒ ì˜¤ë¥˜ í•´ê²°**: ì´ì œ ëª¨ë“  \`require()\` ê²½ë¡œì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ì´ \`COPY src src\` ë‹¨ê³„ì—ì„œ ì»¨í…Œì´ë„ˆì— ì™„ë²½í•˜ê²Œ ë³µì‚¬ë  ê²ƒì…ë‹ˆë‹¤."
echo ""
echo "ë‹¤ìŒ ëª…ë ¹ì„ ìˆœì„œëŒ€ë¡œ ì‹¤í–‰í•˜ì„¸ìš”:"
echo "1. **ì¤‘ìš”**: ì„¤ì • íŒŒì¼ í™•ì¸ ë° API í‚¤ ì…ë ¥: cd ${PROJECT_NAME} && vi volumes/config/config.json"
echo "2. ë¹Œë“œ: ./scripts/build.sh (ìë™ìœ¼ë¡œ \`--no-cache\` ì ìš©)"
echo "3. ì‹¤í–‰: ./scripts/start.sh"
echo "================================================================="
